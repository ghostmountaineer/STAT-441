{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "HW_7.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAhcMdwVklIW"
      },
      "source": [
        "# Homework 7 \n",
        "\n",
        "## Instructions\n",
        "\n",
        "This assignment is due Monday, June 10 by 7pm. Please turn in a copy (either .pdf or .ipynb file) of this notebook as HW7 in Blackboard after you've completed it.\n",
        "\n",
        "The purpose of this assignment is two-fold - to review the basic mathematics for training simple neural networks and ensembles and to explore the vast amount of code and trained models available on Kaggle.  Hopefully the first six homeworks have given you a firm foundation for understanding the mathematics of basic machine learning techniques.  After you finish this class, a great resource to continue improving your machine learning skills and learn about the latest techniques is Kaggle.com, which hosts international machine learning competitions, shared code, tutorials, and free remote computing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWegYI6IklIb"
      },
      "source": [
        "## 1. Conceptual Problem\n",
        "Do problem 5 in Chapter 8.4 on page 332 of the [ISL textbook](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf).  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZsRrgJvklIc"
      },
      "source": [
        "## 2. Ensemble Learning\n",
        "\n",
        "XGBoost is a popular boosting method on Kaggle. [Here's](https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/) a nice summery with additional resources. \n",
        "\n",
        "Go to the \"kernels\" tab of the Titanic Competition in Kaggle and search for \"xgboost python\" to find someone else's code.  \"Fork\" this code and run it in your own account. To do: write a brief description of the final model and give it's Kaggle score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRtYtAZmklIf"
      },
      "source": [
        "## 3. Neural Network\n",
        "\n",
        "[Here's](https://github.com/fchollet/keras-resources) a set of tutorials by its author on using [keras](https://keras.io/getting-started/sequential-model-guide/), a high level library, to fit a simple neural net in R. After reviewing it, go to the \"kernels\" tab of the Titanic Competition in Kaggle and search for \"keras r\" to find someone else's code for a neural network.  \"Fork\" this code and run it in your own account. To do: write a brief description of the final model and give it's Kaggle score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akBY8XcdklIh"
      },
      "source": [
        "## 4. Another neural network\n",
        "\n",
        "Suppose a forward feed neural network is to be used to classify a quantitative variable Y based on three predictors $X_1$, $X_2$ and $X_3$. Suppose that a single hidden layer is used with two neurons $Z_1$ and $Z_2$, each with the Relu activation function. \n",
        "\n",
        "a. Draw the network diagram for the model described above, labeling all nodes with the name of a variable and all connections with the name of its weight (i.e. its model coefficient $\\beta$ with sub and superscripts as needed). \n",
        "\n",
        "b. Assuming that the model will be trained to minimize mean squared error on a training dataset, compute the update step in gradient descent.  That is, compute the gradient of the mean squared error and plug it into $\\beta_i = \\beta_{i-1} - \\alpha \\nabla L(\\beta_{i-1})$ where $L(\\beta) = \\frac{1}{n}\\sum{(y_i-\\hat{y_i})^2}$. "
      ]
    }
  ]
}